{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitminiconda3virtualenv7f2e9c966dbc4598b4ce375409bf61d2",
   "display_name": "Python 3.7.4 64-bit ('miniconda3': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook's aim is to be used as a point of comparisson for the C++ code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns for the data frame\n",
    "col_names = ['id', 'diagnosis']\n",
    "cell_feat = ['rad', 'tex', 'per', 'area', 'smooth', 'compact', 'concav', 'concav_pt', 'sym', 'frac']\n",
    "for feat in cell_feat:\n",
    "    col_names.append(feat+'_mean')\n",
    "    col_names.append(feat+'_se')\n",
    "    col_names.append(feat+'_worse')\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('../../data/cancer/wdbc.data', names=col_names)\n",
    "\n",
    "# Transform the diagnosis to (0,1)\n",
    "df.loc[df['diagnosis']=='M','diagnosis'] = 1 # Malignant\n",
    "df.loc[df['diagnosis']=='B','diagnosis'] = 0 # Bening\n",
    "df['diagnosis'] = df['diagnosis'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>diagnosis</th>\n      <th>rad_mean</th>\n      <th>rad_se</th>\n      <th>rad_worse</th>\n      <th>tex_mean</th>\n      <th>tex_se</th>\n      <th>tex_worse</th>\n      <th>per_mean</th>\n      <th>per_se</th>\n      <th>...</th>\n      <th>concav_worse</th>\n      <th>concav_pt_mean</th>\n      <th>concav_pt_se</th>\n      <th>concav_pt_worse</th>\n      <th>sym_mean</th>\n      <th>sym_se</th>\n      <th>sym_worse</th>\n      <th>frac_mean</th>\n      <th>frac_se</th>\n      <th>frac_worse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>842302</td>\n      <td>1</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>...</td>\n      <td>25.38</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.26540</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>842517</td>\n      <td>1</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>...</td>\n      <td>24.99</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.18600</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>84300903</td>\n      <td>1</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>...</td>\n      <td>23.57</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.24300</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>84348301</td>\n      <td>1</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>...</td>\n      <td>14.91</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.25750</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>84358402</td>\n      <td>1</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>...</td>\n      <td>22.54</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.16250</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>843786</td>\n      <td>1</td>\n      <td>12.45</td>\n      <td>15.70</td>\n      <td>82.57</td>\n      <td>477.1</td>\n      <td>0.12780</td>\n      <td>0.17000</td>\n      <td>0.15780</td>\n      <td>0.08089</td>\n      <td>...</td>\n      <td>15.47</td>\n      <td>23.75</td>\n      <td>103.40</td>\n      <td>741.6</td>\n      <td>0.1791</td>\n      <td>0.5249</td>\n      <td>0.5355</td>\n      <td>0.17410</td>\n      <td>0.3985</td>\n      <td>0.12440</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>844359</td>\n      <td>1</td>\n      <td>18.25</td>\n      <td>19.98</td>\n      <td>119.60</td>\n      <td>1040.0</td>\n      <td>0.09463</td>\n      <td>0.10900</td>\n      <td>0.11270</td>\n      <td>0.07400</td>\n      <td>...</td>\n      <td>22.88</td>\n      <td>27.66</td>\n      <td>153.20</td>\n      <td>1606.0</td>\n      <td>0.1442</td>\n      <td>0.2576</td>\n      <td>0.3784</td>\n      <td>0.19320</td>\n      <td>0.3063</td>\n      <td>0.08368</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>84458202</td>\n      <td>1</td>\n      <td>13.71</td>\n      <td>20.83</td>\n      <td>90.20</td>\n      <td>577.9</td>\n      <td>0.11890</td>\n      <td>0.16450</td>\n      <td>0.09366</td>\n      <td>0.05985</td>\n      <td>...</td>\n      <td>17.06</td>\n      <td>28.14</td>\n      <td>110.60</td>\n      <td>897.0</td>\n      <td>0.1654</td>\n      <td>0.3682</td>\n      <td>0.2678</td>\n      <td>0.15560</td>\n      <td>0.3196</td>\n      <td>0.11510</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>844981</td>\n      <td>1</td>\n      <td>13.00</td>\n      <td>21.82</td>\n      <td>87.50</td>\n      <td>519.8</td>\n      <td>0.12730</td>\n      <td>0.19320</td>\n      <td>0.18590</td>\n      <td>0.09353</td>\n      <td>...</td>\n      <td>15.49</td>\n      <td>30.73</td>\n      <td>106.20</td>\n      <td>739.3</td>\n      <td>0.1703</td>\n      <td>0.5401</td>\n      <td>0.5390</td>\n      <td>0.20600</td>\n      <td>0.4378</td>\n      <td>0.10720</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>84501001</td>\n      <td>1</td>\n      <td>12.46</td>\n      <td>24.04</td>\n      <td>83.97</td>\n      <td>475.9</td>\n      <td>0.11860</td>\n      <td>0.23960</td>\n      <td>0.22730</td>\n      <td>0.08543</td>\n      <td>...</td>\n      <td>15.09</td>\n      <td>40.68</td>\n      <td>97.65</td>\n      <td>711.4</td>\n      <td>0.1853</td>\n      <td>1.0580</td>\n      <td>1.1050</td>\n      <td>0.22100</td>\n      <td>0.4366</td>\n      <td>0.20750</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>845636</td>\n      <td>1</td>\n      <td>16.02</td>\n      <td>23.24</td>\n      <td>102.70</td>\n      <td>797.8</td>\n      <td>0.08206</td>\n      <td>0.06669</td>\n      <td>0.03299</td>\n      <td>0.03323</td>\n      <td>...</td>\n      <td>19.19</td>\n      <td>33.88</td>\n      <td>123.80</td>\n      <td>1150.0</td>\n      <td>0.1181</td>\n      <td>0.1551</td>\n      <td>0.1459</td>\n      <td>0.09975</td>\n      <td>0.2948</td>\n      <td>0.08452</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>84610002</td>\n      <td>1</td>\n      <td>15.78</td>\n      <td>17.89</td>\n      <td>103.60</td>\n      <td>781.0</td>\n      <td>0.09710</td>\n      <td>0.12920</td>\n      <td>0.09954</td>\n      <td>0.06606</td>\n      <td>...</td>\n      <td>20.42</td>\n      <td>27.28</td>\n      <td>136.50</td>\n      <td>1299.0</td>\n      <td>0.1396</td>\n      <td>0.5609</td>\n      <td>0.3965</td>\n      <td>0.18100</td>\n      <td>0.3792</td>\n      <td>0.10480</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>846226</td>\n      <td>1</td>\n      <td>19.17</td>\n      <td>24.80</td>\n      <td>132.40</td>\n      <td>1123.0</td>\n      <td>0.09740</td>\n      <td>0.24580</td>\n      <td>0.20650</td>\n      <td>0.11180</td>\n      <td>...</td>\n      <td>20.96</td>\n      <td>29.94</td>\n      <td>151.70</td>\n      <td>1332.0</td>\n      <td>0.1037</td>\n      <td>0.3903</td>\n      <td>0.3639</td>\n      <td>0.17670</td>\n      <td>0.3176</td>\n      <td>0.10230</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>846381</td>\n      <td>1</td>\n      <td>15.85</td>\n      <td>23.95</td>\n      <td>103.70</td>\n      <td>782.7</td>\n      <td>0.08401</td>\n      <td>0.10020</td>\n      <td>0.09938</td>\n      <td>0.05364</td>\n      <td>...</td>\n      <td>16.84</td>\n      <td>27.66</td>\n      <td>112.00</td>\n      <td>876.5</td>\n      <td>0.1131</td>\n      <td>0.1924</td>\n      <td>0.2322</td>\n      <td>0.11190</td>\n      <td>0.2809</td>\n      <td>0.06287</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>84667401</td>\n      <td>1</td>\n      <td>13.73</td>\n      <td>22.61</td>\n      <td>93.60</td>\n      <td>578.3</td>\n      <td>0.11310</td>\n      <td>0.22930</td>\n      <td>0.21280</td>\n      <td>0.08025</td>\n      <td>...</td>\n      <td>15.03</td>\n      <td>32.01</td>\n      <td>108.80</td>\n      <td>697.7</td>\n      <td>0.1651</td>\n      <td>0.7725</td>\n      <td>0.6943</td>\n      <td>0.22080</td>\n      <td>0.3596</td>\n      <td>0.14310</td>\n    </tr>\n  </tbody>\n</table>\n<p>15 rows × 32 columns</p>\n</div>",
      "text/plain": "          id  diagnosis  rad_mean  rad_se  rad_worse  tex_mean   tex_se  \\\n0     842302          1     17.99   10.38     122.80    1001.0  0.11840   \n1     842517          1     20.57   17.77     132.90    1326.0  0.08474   \n2   84300903          1     19.69   21.25     130.00    1203.0  0.10960   \n3   84348301          1     11.42   20.38      77.58     386.1  0.14250   \n4   84358402          1     20.29   14.34     135.10    1297.0  0.10030   \n5     843786          1     12.45   15.70      82.57     477.1  0.12780   \n6     844359          1     18.25   19.98     119.60    1040.0  0.09463   \n7   84458202          1     13.71   20.83      90.20     577.9  0.11890   \n8     844981          1     13.00   21.82      87.50     519.8  0.12730   \n9   84501001          1     12.46   24.04      83.97     475.9  0.11860   \n10    845636          1     16.02   23.24     102.70     797.8  0.08206   \n11  84610002          1     15.78   17.89     103.60     781.0  0.09710   \n12    846226          1     19.17   24.80     132.40    1123.0  0.09740   \n13    846381          1     15.85   23.95     103.70     782.7  0.08401   \n14  84667401          1     13.73   22.61      93.60     578.3  0.11310   \n\n    tex_worse  per_mean   per_se  ...  concav_worse  concav_pt_mean  \\\n0     0.27760   0.30010  0.14710  ...         25.38           17.33   \n1     0.07864   0.08690  0.07017  ...         24.99           23.41   \n2     0.15990   0.19740  0.12790  ...         23.57           25.53   \n3     0.28390   0.24140  0.10520  ...         14.91           26.50   \n4     0.13280   0.19800  0.10430  ...         22.54           16.67   \n5     0.17000   0.15780  0.08089  ...         15.47           23.75   \n6     0.10900   0.11270  0.07400  ...         22.88           27.66   \n7     0.16450   0.09366  0.05985  ...         17.06           28.14   \n8     0.19320   0.18590  0.09353  ...         15.49           30.73   \n9     0.23960   0.22730  0.08543  ...         15.09           40.68   \n10    0.06669   0.03299  0.03323  ...         19.19           33.88   \n11    0.12920   0.09954  0.06606  ...         20.42           27.28   \n12    0.24580   0.20650  0.11180  ...         20.96           29.94   \n13    0.10020   0.09938  0.05364  ...         16.84           27.66   \n14    0.22930   0.21280  0.08025  ...         15.03           32.01   \n\n    concav_pt_se  concav_pt_worse  sym_mean  sym_se  sym_worse  frac_mean  \\\n0         184.60           2019.0    0.1622  0.6656     0.7119    0.26540   \n1         158.80           1956.0    0.1238  0.1866     0.2416    0.18600   \n2         152.50           1709.0    0.1444  0.4245     0.4504    0.24300   \n3          98.87            567.7    0.2098  0.8663     0.6869    0.25750   \n4         152.20           1575.0    0.1374  0.2050     0.4000    0.16250   \n5         103.40            741.6    0.1791  0.5249     0.5355    0.17410   \n6         153.20           1606.0    0.1442  0.2576     0.3784    0.19320   \n7         110.60            897.0    0.1654  0.3682     0.2678    0.15560   \n8         106.20            739.3    0.1703  0.5401     0.5390    0.20600   \n9          97.65            711.4    0.1853  1.0580     1.1050    0.22100   \n10        123.80           1150.0    0.1181  0.1551     0.1459    0.09975   \n11        136.50           1299.0    0.1396  0.5609     0.3965    0.18100   \n12        151.70           1332.0    0.1037  0.3903     0.3639    0.17670   \n13        112.00            876.5    0.1131  0.1924     0.2322    0.11190   \n14        108.80            697.7    0.1651  0.7725     0.6943    0.22080   \n\n    frac_se  frac_worse  \n0    0.4601     0.11890  \n1    0.2750     0.08902  \n2    0.3613     0.08758  \n3    0.6638     0.17300  \n4    0.2364     0.07678  \n5    0.3985     0.12440  \n6    0.3063     0.08368  \n7    0.3196     0.11510  \n8    0.4378     0.10720  \n9    0.4366     0.20750  \n10   0.2948     0.08452  \n11   0.3792     0.10480  \n12   0.3176     0.10230  \n13   0.2809     0.06287  \n14   0.3596     0.14310  \n\n[15 rows x 32 columns]"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>diagnosis</th>\n      <th>rad_mean</th>\n      <th>rad_se</th>\n      <th>rad_worse</th>\n      <th>tex_mean</th>\n      <th>tex_se</th>\n      <th>tex_worse</th>\n      <th>per_mean</th>\n      <th>per_se</th>\n      <th>...</th>\n      <th>concav_worse</th>\n      <th>concav_pt_mean</th>\n      <th>concav_pt_se</th>\n      <th>concav_pt_worse</th>\n      <th>sym_mean</th>\n      <th>sym_se</th>\n      <th>sym_worse</th>\n      <th>frac_mean</th>\n      <th>frac_se</th>\n      <th>frac_worse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5.690000e+02</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>...</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.037183e+07</td>\n      <td>0.372583</td>\n      <td>14.127292</td>\n      <td>19.289649</td>\n      <td>91.969033</td>\n      <td>654.889104</td>\n      <td>0.096360</td>\n      <td>0.104341</td>\n      <td>0.088799</td>\n      <td>0.048919</td>\n      <td>...</td>\n      <td>16.269190</td>\n      <td>25.677223</td>\n      <td>107.261213</td>\n      <td>880.583128</td>\n      <td>0.132369</td>\n      <td>0.254265</td>\n      <td>0.272188</td>\n      <td>0.114606</td>\n      <td>0.290076</td>\n      <td>0.083946</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.250206e+08</td>\n      <td>0.483918</td>\n      <td>3.524049</td>\n      <td>4.301036</td>\n      <td>24.298981</td>\n      <td>351.914129</td>\n      <td>0.014064</td>\n      <td>0.052813</td>\n      <td>0.079720</td>\n      <td>0.038803</td>\n      <td>...</td>\n      <td>4.833242</td>\n      <td>6.146258</td>\n      <td>33.602542</td>\n      <td>569.356993</td>\n      <td>0.022832</td>\n      <td>0.157336</td>\n      <td>0.208624</td>\n      <td>0.065732</td>\n      <td>0.061867</td>\n      <td>0.018061</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>8.670000e+03</td>\n      <td>0.000000</td>\n      <td>6.981000</td>\n      <td>9.710000</td>\n      <td>43.790000</td>\n      <td>143.500000</td>\n      <td>0.052630</td>\n      <td>0.019380</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>7.930000</td>\n      <td>12.020000</td>\n      <td>50.410000</td>\n      <td>185.200000</td>\n      <td>0.071170</td>\n      <td>0.027290</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.156500</td>\n      <td>0.055040</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>8.692180e+05</td>\n      <td>0.000000</td>\n      <td>11.700000</td>\n      <td>16.170000</td>\n      <td>75.170000</td>\n      <td>420.300000</td>\n      <td>0.086370</td>\n      <td>0.064920</td>\n      <td>0.029560</td>\n      <td>0.020310</td>\n      <td>...</td>\n      <td>13.010000</td>\n      <td>21.080000</td>\n      <td>84.110000</td>\n      <td>515.300000</td>\n      <td>0.116600</td>\n      <td>0.147200</td>\n      <td>0.114500</td>\n      <td>0.064930</td>\n      <td>0.250400</td>\n      <td>0.071460</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>9.060240e+05</td>\n      <td>0.000000</td>\n      <td>13.370000</td>\n      <td>18.840000</td>\n      <td>86.240000</td>\n      <td>551.100000</td>\n      <td>0.095870</td>\n      <td>0.092630</td>\n      <td>0.061540</td>\n      <td>0.033500</td>\n      <td>...</td>\n      <td>14.970000</td>\n      <td>25.410000</td>\n      <td>97.660000</td>\n      <td>686.500000</td>\n      <td>0.131300</td>\n      <td>0.211900</td>\n      <td>0.226700</td>\n      <td>0.099930</td>\n      <td>0.282200</td>\n      <td>0.080040</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8.813129e+06</td>\n      <td>1.000000</td>\n      <td>15.780000</td>\n      <td>21.800000</td>\n      <td>104.100000</td>\n      <td>782.700000</td>\n      <td>0.105300</td>\n      <td>0.130400</td>\n      <td>0.130700</td>\n      <td>0.074000</td>\n      <td>...</td>\n      <td>18.790000</td>\n      <td>29.720000</td>\n      <td>125.400000</td>\n      <td>1084.000000</td>\n      <td>0.146000</td>\n      <td>0.339100</td>\n      <td>0.382900</td>\n      <td>0.161400</td>\n      <td>0.317900</td>\n      <td>0.092080</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>9.113205e+08</td>\n      <td>1.000000</td>\n      <td>28.110000</td>\n      <td>39.280000</td>\n      <td>188.500000</td>\n      <td>2501.000000</td>\n      <td>0.163400</td>\n      <td>0.345400</td>\n      <td>0.426800</td>\n      <td>0.201200</td>\n      <td>...</td>\n      <td>36.040000</td>\n      <td>49.540000</td>\n      <td>251.200000</td>\n      <td>4254.000000</td>\n      <td>0.222600</td>\n      <td>1.058000</td>\n      <td>1.252000</td>\n      <td>0.291000</td>\n      <td>0.663800</td>\n      <td>0.207500</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 32 columns</p>\n</div>",
      "text/plain": "                 id   diagnosis    rad_mean      rad_se   rad_worse  \\\ncount  5.690000e+02  569.000000  569.000000  569.000000  569.000000   \nmean   3.037183e+07    0.372583   14.127292   19.289649   91.969033   \nstd    1.250206e+08    0.483918    3.524049    4.301036   24.298981   \nmin    8.670000e+03    0.000000    6.981000    9.710000   43.790000   \n25%    8.692180e+05    0.000000   11.700000   16.170000   75.170000   \n50%    9.060240e+05    0.000000   13.370000   18.840000   86.240000   \n75%    8.813129e+06    1.000000   15.780000   21.800000  104.100000   \nmax    9.113205e+08    1.000000   28.110000   39.280000  188.500000   \n\n          tex_mean      tex_se   tex_worse    per_mean      per_se  ...  \\\ncount   569.000000  569.000000  569.000000  569.000000  569.000000  ...   \nmean    654.889104    0.096360    0.104341    0.088799    0.048919  ...   \nstd     351.914129    0.014064    0.052813    0.079720    0.038803  ...   \nmin     143.500000    0.052630    0.019380    0.000000    0.000000  ...   \n25%     420.300000    0.086370    0.064920    0.029560    0.020310  ...   \n50%     551.100000    0.095870    0.092630    0.061540    0.033500  ...   \n75%     782.700000    0.105300    0.130400    0.130700    0.074000  ...   \nmax    2501.000000    0.163400    0.345400    0.426800    0.201200  ...   \n\n       concav_worse  concav_pt_mean  concav_pt_se  concav_pt_worse  \\\ncount    569.000000      569.000000    569.000000       569.000000   \nmean      16.269190       25.677223    107.261213       880.583128   \nstd        4.833242        6.146258     33.602542       569.356993   \nmin        7.930000       12.020000     50.410000       185.200000   \n25%       13.010000       21.080000     84.110000       515.300000   \n50%       14.970000       25.410000     97.660000       686.500000   \n75%       18.790000       29.720000    125.400000      1084.000000   \nmax       36.040000       49.540000    251.200000      4254.000000   \n\n         sym_mean      sym_se   sym_worse   frac_mean     frac_se  frac_worse  \ncount  569.000000  569.000000  569.000000  569.000000  569.000000  569.000000  \nmean     0.132369    0.254265    0.272188    0.114606    0.290076    0.083946  \nstd      0.022832    0.157336    0.208624    0.065732    0.061867    0.018061  \nmin      0.071170    0.027290    0.000000    0.000000    0.156500    0.055040  \n25%      0.116600    0.147200    0.114500    0.064930    0.250400    0.071460  \n50%      0.131300    0.211900    0.226700    0.099930    0.282200    0.080040  \n75%      0.146000    0.339100    0.382900    0.161400    0.317900    0.092080  \nmax      0.222600    1.058000    1.252000    0.291000    0.663800    0.207500  \n\n[8 rows x 32 columns]"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 569 entries, 0 to 568\nData columns (total 32 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   id               569 non-null    int64  \n 1   diagnosis        569 non-null    int64  \n 2   rad_mean         569 non-null    float64\n 3   rad_se           569 non-null    float64\n 4   rad_worse        569 non-null    float64\n 5   tex_mean         569 non-null    float64\n 6   tex_se           569 non-null    float64\n 7   tex_worse        569 non-null    float64\n 8   per_mean         569 non-null    float64\n 9   per_se           569 non-null    float64\n 10  per_worse        569 non-null    float64\n 11  area_mean        569 non-null    float64\n 12  area_se          569 non-null    float64\n 13  area_worse       569 non-null    float64\n 14  smooth_mean      569 non-null    float64\n 15  smooth_se        569 non-null    float64\n 16  smooth_worse     569 non-null    float64\n 17  compact_mean     569 non-null    float64\n 18  compact_se       569 non-null    float64\n 19  compact_worse    569 non-null    float64\n 20  concav_mean      569 non-null    float64\n 21  concav_se        569 non-null    float64\n 22  concav_worse     569 non-null    float64\n 23  concav_pt_mean   569 non-null    float64\n 24  concav_pt_se     569 non-null    float64\n 25  concav_pt_worse  569 non-null    float64\n 26  sym_mean         569 non-null    float64\n 27  sym_se           569 non-null    float64\n 28  sym_worse        569 non-null    float64\n 29  frac_mean        569 non-null    float64\n 30  frac_se          569 non-null    float64\n 31  frac_worse       569 non-null    float64\ndtypes: float64(30), int64(2)\nmemory usage: 142.4 KB\n"
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To be consistent with the test, we use the first n_train values to train, and only the rest to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "500  gives a prediction of  0 ( true =  0 )\n501  gives a prediction of  1 ( true =  1 )\n502  gives a prediction of  0 ( true =  0 )\n503  gives a prediction of  1 ( true =  1 )\n504  gives a prediction of  0 ( true =  0 )\n505  gives a prediction of  0 ( true =  0 )\n506  gives a prediction of  0 ( true =  0 )\n507  gives a prediction of  0 ( true =  0 )\n508  gives a prediction of  1 ( true =  0 )\n509  gives a prediction of  1 ( true =  1 )\n510  gives a prediction of  0 ( true =  0 )\n511  gives a prediction of  0 ( true =  0 )\n512  gives a prediction of  1 ( true =  1 )\n513  gives a prediction of  0 ( true =  0 )\n514  gives a prediction of  1 ( true =  1 )\n515  gives a prediction of  0 ( true =  0 )\n516  gives a prediction of  1 ( true =  1 )\n517  gives a prediction of  1 ( true =  1 )\n518  gives a prediction of  0 ( true =  0 )\n519  gives a prediction of  0 ( true =  0 )\n520  gives a prediction of  0 ( true =  0 )\n521  gives a prediction of  1 ( true =  1 )\n522  gives a prediction of  0 ( true =  0 )\n523  gives a prediction of  0 ( true =  0 )\n524  gives a prediction of  0 ( true =  0 )\n525  gives a prediction of  0 ( true =  0 )\n526  gives a prediction of  0 ( true =  0 )\n527  gives a prediction of  0 ( true =  0 )\n528  gives a prediction of  0 ( true =  0 )\n529  gives a prediction of  0 ( true =  0 )\n530  gives a prediction of  0 ( true =  0 )\n531  gives a prediction of  0 ( true =  0 )\n532  gives a prediction of  0 ( true =  0 )\n533  gives a prediction of  1 ( true =  1 )\n534  gives a prediction of  0 ( true =  0 )\n535  gives a prediction of  1 ( true =  1 )\n536  gives a prediction of  0 ( true =  1 )\n537  gives a prediction of  0 ( true =  0 )\n538  gives a prediction of  0 ( true =  0 )\n539  gives a prediction of  0 ( true =  0 )\n540  gives a prediction of  0 ( true =  0 )\n541  gives a prediction of  1 ( true =  0 )\n542  gives a prediction of  0 ( true =  0 )\n543  gives a prediction of  0 ( true =  0 )\n544  gives a prediction of  0 ( true =  0 )\n545  gives a prediction of  0 ( true =  0 )\n546  gives a prediction of  0 ( true =  0 )\n547  gives a prediction of  0 ( true =  0 )\n548  gives a prediction of  0 ( true =  0 )\n549  gives a prediction of  0 ( true =  0 )\n550  gives a prediction of  0 ( true =  0 )\n551  gives a prediction of  0 ( true =  0 )\n552  gives a prediction of  0 ( true =  0 )\n553  gives a prediction of  0 ( true =  0 )\n554  gives a prediction of  0 ( true =  0 )\n555  gives a prediction of  0 ( true =  0 )\n556  gives a prediction of  0 ( true =  0 )\n557  gives a prediction of  0 ( true =  0 )\n558  gives a prediction of  0 ( true =  0 )\n559  gives a prediction of  0 ( true =  0 )\n560  gives a prediction of  0 ( true =  0 )\n561  gives a prediction of  0 ( true =  0 )\n562  gives a prediction of  1 ( true =  1 )\n563  gives a prediction of  1 ( true =  1 )\n564  gives a prediction of  1 ( true =  1 )\n565  gives a prediction of  1 ( true =  1 )\n566  gives a prediction of  1 ( true =  1 )\n567  gives a prediction of  1 ( true =  1 )\n568  gives a prediction of  0 ( true =  0 )\n"
    }
   ],
   "source": [
    "n_train = 500\n",
    "\n",
    "# New instance of the classifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5, algorithm='brute')\n",
    "\n",
    "X_test  = df.drop(labels=['id', 'diagnosis'],axis=1)[n_train:]\n",
    "X_train = df.drop(labels=['id', 'diagnosis'],axis=1)[0:n_train]\n",
    "y_train = df['diagnosis'][0:n_train]\n",
    "\n",
    "neigh.fit(X_train,y_train)\n",
    "\n",
    "predictions = neigh.predict(X_test)\n",
    "\n",
    "results_py = []\n",
    "for ind, prediction in zip(X_test.index,predictions):\n",
    "    print(ind, ' gives a prediction of ', prediction, '( true = ', df['diagnosis'][ind], ')')\n",
    "    results_py.append( [ind, prediction] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we copy the results from the C++ output\n",
    "results_cpp = [\n",
    "    [500, 0],\n",
    "    [501, 1],\n",
    "    [502, 0],\n",
    "    [503, 1],\n",
    "    [504, 0],\n",
    "    [505, 0],\n",
    "    [506, 0],\n",
    "    [507, 0],\n",
    "    [508, 1],\n",
    "    [509, 1],\n",
    "    [510, 0],\n",
    "    [511, 0],\n",
    "    [512, 1],\n",
    "    [513, 0],\n",
    "    [514, 1],\n",
    "    [515, 0],\n",
    "    [516, 1],\n",
    "    [517, 1],\n",
    "    [518, 0],\n",
    "    [519, 0],\n",
    "    [520, 0],\n",
    "    [521, 1],\n",
    "    [522, 0],\n",
    "    [523, 0],\n",
    "    [524, 0],\n",
    "    [525, 0],\n",
    "    [526, 0],\n",
    "    [527, 0],\n",
    "    [528, 0],\n",
    "    [529, 0],\n",
    "    [530, 0],\n",
    "    [531, 0],\n",
    "    [532, 0],\n",
    "    [533, 1],\n",
    "    [534, 0],\n",
    "    [535, 1],\n",
    "    [536, 0],\n",
    "    [537, 0],\n",
    "    [538, 0],\n",
    "    [539, 0],\n",
    "    [540, 0],\n",
    "    [541, 1],\n",
    "    [542, 0],\n",
    "    [543, 0],\n",
    "    [544, 0],\n",
    "    [545, 0],\n",
    "    [546, 0],\n",
    "    [547, 0],\n",
    "    [548, 0],\n",
    "    [549, 0],\n",
    "    [550, 0],\n",
    "    [551, 0],\n",
    "    [552, 0],\n",
    "    [553, 0],\n",
    "    [554, 0],\n",
    "    [555, 0],\n",
    "    [556, 0],\n",
    "    [557, 0],\n",
    "    [558, 0],\n",
    "    [559, 0],\n",
    "    [560, 0],\n",
    "    [561, 0],\n",
    "    [562, 1],\n",
    "    [563, 1],\n",
    "    [564, 1],\n",
    "    [565, 1],\n",
    "    [566, 1],\n",
    "    [567, 1],\n",
    "    [568, 0],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Total number of disagreements =  0\n"
    }
   ],
   "source": [
    "# Let's look at the difference between the predictions on C++ and python\n",
    "delta = []\n",
    "for cpp, py in zip(results_cpp, results_py):\n",
    "    delta.append(cpp[1]-py[1])\n",
    "\n",
    "print( 'Total number of disagreements = ', sum([val !=0 for val in delta]) )"
   ]
  }
 ]
}